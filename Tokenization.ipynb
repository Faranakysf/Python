{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#10A2DF> <p style=\"text-align: center;\">&#9733; Natural Language Processing  &#9733;</p></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Instal NLTK toolkit for text processing.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Import NLTK and necessary modules and components.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer, RegexpTokenizer, WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Create a body of text}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I have been working at amazon full-time for more than two years. Yet, I don't have a close relationship with my manager Dr. Smith. \" + \\\n",
    "       \"No work/life balance. \" + \\\n",
    "       \"so realistically you only will have 8 days off a year when you combine the two together. \" + \\\n",
    "       \"I live on the eastside of New York (18mi from office), and my manager isnot flexible on my start time. \" + \\\n",
    "       \"salaries in Minimum wage, Benefits capped to $100,000 per calendar year! \" + \\\n",
    "       \"there is no Principle around treating employees well. Managers' attitude is negative and treat employees with no respect. \"       \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Trained: Convert the text to sentence tokens}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence_tkns = nltk.sent_tokenize(text)\n",
    "Sentence_tkns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Untrained: Create a Punkt sentence tokenizer.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnkt_sntnce_tknzr = nltk.PunktSentenceTokenizer()\n",
    "pnkt_sntnce_tknzr.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Sentence_tkns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Sentence_tkns[0])\n",
    "print(Sentence_tkns[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Tokenize the text by words.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Word_tokenize a specific sentence with index of tokens.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.word_tokenize(Sentence_tkns[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Tokenize all sentences in Sentence_tkns.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word_tkns = [nltk.word_tokenize(Sentence_tkns) for Sentence_tkns in Sentence_tkns]\n",
    "\n",
    "for element in Word_tkns:\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Use Treebank to split standard contractions and split off commas.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBW_tkns = nltk.TreebankWordTokenizer()\n",
    "print(TBW_tkns.tokenize(Sentence_tkns[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Use Regexp Tokenizer to segment text by word characters and remove punctuations.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ptrn_words = r'\\w+' \n",
    "RGX_tkns = nltk.RegexpTokenizer(pattern=Ptrn_words, gaps=False)\n",
    "print(RGX_tkns.tokenize(Sentence_tkns[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Use Regexp Tokenizer to segment text by whitespace characters.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ptrn_whiteSp = r'\\s+'\n",
    "RGX_tkns = nltk.RegexpTokenizer(pattern=Ptrn_whiteSp, gaps=True)\n",
    "print(RGX_tkns.tokenize(Sentence_tkns[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Use Whitespace Tokenizer to explicitly segment text by whitespace characters (space, tab, newline).}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WST = nltk.WhitespaceTokenizer()\n",
    "print(WST.tokenize(Sentence_tkns[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Convert upercase words to lower case.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_tokens(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "print(lower_tokens(nltk.word_tokenize(Sentence_tkns[5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Use string punctuation to remove all punctuation from t.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Create a function that removes punctuation.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a function to remove all punctuation in each token in a list\n",
    "\n",
    "def remove_punct(tokens):\n",
    "    punct_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    return [a for a,b in zip(tokens, [punct_regex.sub('', token) for token in tokens]) if b != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{#10A2DF}{\\text{Use the function for a text.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_punct(lower_tokens(nltk.word_tokenize(Sentence_tkns[6]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
